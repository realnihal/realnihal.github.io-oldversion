I"7ë<h1 id="food-vision-using-an-effecientnetx-and-tensorflow"><strong>Food Vision Using an EffecientNetX and Tensorflow</strong></h1>
<p>We are using the <strong>Food101</strong> standard database to create a deep learning model that can tell the difference and make predictions between 101 classes of food. using all 75,750 training images and 25,250 testing images. We will also use mixed learning to increase the speed of the training process.</p>

<p>Our goal is to beat the original <a href="https://arxiv.org/ftp/arxiv/papers/1606/1606.05675.pdf">DeepFood </a> paper. they have an accuracy of 77.4%.</p>

<p>We will use the power of transfer learning by incorporating the EfficientNetX architecture, using the following steps:</p>

<ol>
  <li>Feature Extraction</li>
  <li>Fine Tuning</li>
</ol>

<p>##Checking for the right GPU
Since we are planning to use mixed learning, we need a compatible GPU.
This model is being trained on Googleâ€™s colab and they provides 3 types of free Nvidia GPUâ€™s.</p>

<ol>
  <li>K80(not compatible)</li>
  <li>P100(not compatible)</li>
  <li>Tesla T4(compatible)</li>
</ol>

<p>Knowing this we need access to an Nvidia Tesla T4(from colab) or any GPU with a compute score of 7+ of our own.</p>

<p>Lets find out our GPU type by the following command.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">nvidia</span><span class="o">-</span><span class="n">smi</span> <span class="o">-</span><span class="n">L</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU 0: Tesla T4 (UUID: GPU-b54d7bc4-6911-4395-1619-170d04e3161d)
</code></pre></div></div>

<p>Great! we have a compatible GPU ie, the Tesla T4. If you do not have a compatible GPU try factory runtime -&gt; factory reset to reset your session.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show the Tensorflow Version (run this in Google Colab) 
</span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="k">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2.4.1
</code></pre></div></div>

<p>There is a known bug with tensorflow 2.5.0 specifically that doesâ€™nt work with mixed learning. To avoid that use the following code to downgrade to version 2.4.1.<a href="https://github.com/tensorflow/tensorflow/issues/49725">For more info</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Downgrade Tensorflow Version (run this in Google Colab) 
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.4</span><span class="p">.</span><span class="mi">1</span>
</code></pre></div></div>

<p>##Importing the Data
We have created some helper functions that we are importing into our project.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="p">.</span><span class="n">githubusercontent</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">realnihal</span><span class="o">/</span><span class="n">Random_Code</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">helper_functions</span><span class="p">.</span><span class="n">py</span>

<span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">create_tensorboard_callback</span><span class="p">,</span> <span class="n">plot_loss_curves</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--2021-08-05 05:15:53--  https://raw.githubusercontent.com/realnihal/Random_Code/master/helper_functions.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10510 (10K) [text/plain]
Saving to: â€˜helper_functions.pyâ€™

helper_functions.py 100%[===================&gt;]  10.26K  --.-KB/s    in 0s      

2021-08-05 05:15:53 (89.1 MB/s) - â€˜helper_functions.pyâ€™ saved [10510/10510]
</code></pre></div></div>

<p>The dataset <strong>Food101</strong> in available in to download from tensorflow datasets. You can find out more about this from here <a href="https://www.tensorflow.org/datasets">Tensorflow Datasets</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#importing tfds
</span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="n">tfds</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#checking the name of the dataset in present within tfds
</span><span class="n">datasets_list</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">list_builders</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"food101"</span> <span class="ow">in</span> <span class="n">datasets_list</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div>

<p>This may take a few minutes since the data is so large.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Loading the dataset
</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">),</span> <span class="n">ds_info</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'food101'</span><span class="p">,</span>
                                             <span class="n">split</span><span class="o">=</span><span class="p">[</span><span class="s">'train'</span><span class="p">,</span><span class="s">'validation'</span><span class="p">],</span>
                                             <span class="n">shuffle_files</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                             <span class="n">as_supervised</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                             <span class="n">with_info</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1mDownloading and preparing dataset food101/2.0.0 (download: 4.65 GiB, generated: Unknown size, total: 4.65 GiB) to /root/tensorflow_datasets/food101/2.0.0...[0m



HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progreâ€¦



HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressStyâ€¦



HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, stylâ€¦











HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))


Shuffling and writing examples to /root/tensorflow_datasets/food101/2.0.0.incompleteH1HN4N/food101-train.tfrecord



HBox(children=(FloatProgress(value=0.0, max=75750.0), HTML(value='')))



HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))


Shuffling and writing examples to /root/tensorflow_datasets/food101/2.0.0.incompleteH1HN4N/food101-validation.tfrecord



HBox(children=(FloatProgress(value=0.0, max=25250.0), HTML(value='')))


[1mDataset food101 downloaded and prepared to /root/tensorflow_datasets/food101/2.0.0. Subsequent calls will reuse this data.[0m
</code></pre></div></div>

<p>##Data exploration</p>

<p>lets see what the downloaded data contains. This allows us to get a more better idea of how to preprocess the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Features of Food101 from TFDS
</span><span class="n">ds_info</span><span class="p">.</span><span class="n">features</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=101),
})
</code></pre></div></div>

<p>First thing we notice is that the image is in the <strong>uint8</strong> format we need to convert it into <strong>float32</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Printing a few class names
</span><span class="n">class_names</span> <span class="o">=</span> <span class="n">ds_info</span><span class="p">.</span><span class="n">features</span><span class="p">[</span><span class="s">"label"</span><span class="p">].</span><span class="n">names</span>
<span class="n">class_names</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span> <span class="c1">#only print 20 to avoid spamming the output
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['apple_pie',
 'baby_back_ribs',
 'baklava',
 'beef_carpaccio',
 'beef_tartare',
 'beet_salad',
 'beignets',
 'bibimbap',
 'bread_pudding',
 'breakfast_burrito']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Lets take one sample to dive deeper and view our data
</span><span class="n">train_one_sample</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_one_sample</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;TakeDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Printing the image features
</span><span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">train_one_sample</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'''
  Image shape: </span><span class="si">{</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">
  Iimage datatype: </span><span class="si">{</span><span class="n">image</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">
  Target class from Food101 (tensor form): </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s">
  Class name (str form): </span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">.</span><span class="n">numpy</span><span class="p">()]</span><span class="si">}</span><span class="s">
  '''</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Image shape: (512, 512, 3)
  Iimage datatype: &lt;dtype: 'uint8'&gt;
  Target class from Food101 (tensor form): 25
  Class name (str form): club_sandwich
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Checking whether the image is normalized
</span><span class="n">image</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;tf.Tensor: shape=(512, 512, 3), dtype=uint8, numpy=
array([[[135, 156, 175],
        [125, 148, 166],
        [114, 136, 159],
        ...,
        [ 26,   5,  12],
        [ 26,   3,  11],
        [ 27,   4,  12]],

       [[128, 150, 171],
        [115, 140, 160],
        [102, 127, 149],
        ...,
        [ 28,   7,  14],
        [ 29,   6,  14],
        [ 30,   7,  15]],

       [[112, 139, 160],
        [ 99, 127, 148],
        [ 87, 115, 137],
        ...,
        [ 29,   6,  16],
        [ 31,   5,  16],
        [ 32,   6,  17]],

       ...,

       [[ 48,  47,  53],
        [ 53,  52,  58],
        [ 52,  51,  59],
        ...,
        [111,  99,  99],
        [108,  98,  97],
        [106,  96,  97]],

       [[ 44,  45,  47],
        [ 48,  49,  51],
        [ 46,  47,  51],
        ...,
        [108,  96,  98],
        [105,  94,  98],
        [102,  93,  96]],

       [[ 40,  42,  41],
        [ 45,  47,  46],
        [ 44,  45,  49],
        ...,
        [105,  95,  96],
        [104,  93,  99],
        [100,  91,  96]]], dtype=uint8)&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(&lt;tf.Tensor: shape=(), dtype=uint8, numpy=0&gt;,
 &lt;tf.Tensor: shape=(), dtype=uint8, numpy=255&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Plotting an image from the dataset to check if our labels are correct
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">.</span><span class="n">numpy</span><span class="p">()])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(-0.5, 511.5, 511.5, -0.5)
</code></pre></div></div>

<p><img src="output_22_1.png" alt="png" /></p>

<p>##Preprocessing the data</p>

<p>from our initial data exploration we found that we need to do the following things:</p>

<ol>
  <li>Reshaping our images to a standard size - [255,255]</li>
  <li>Converting our images into float32.</li>
  <li>Shuffling them again.</li>
  <li>Set up batches to ensure we dont run out of memory.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make a function for preprocessing images
</span><span class="k">def</span> <span class="nf">preprocess_img</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">img_shape</span><span class="o">=</span><span class="mi">224</span><span class="p">):</span>
  <span class="s">"""
  Converts image datatype from 'uint8' -&gt; 'float32' and reshapes image to
  [img_shape, img_shape, color_channels]
  """</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="n">img_shape</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">])</span> <span class="c1"># reshape to img_shape
</span>  <span class="c1">#image = image/255.0 (not required)
</span>  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">label</span> <span class="c1"># return (float32_image, label) tuple
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Using our preprocess function to test on the sample image
</span><span class="n">preprocessed_img</span> <span class="o">=</span> <span class="n">preprocess_img</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Image before preprocessing:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">image</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s">...,</span><span class="se">\n</span><span class="s">Shape: </span><span class="si">{</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">,</span><span class="se">\n</span><span class="s">Datatype: </span><span class="si">{</span><span class="n">image</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Image after preprocessing:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">preprocessed_img</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s">...,</span><span class="se">\n</span><span class="s">Shape: </span><span class="si">{</span><span class="n">preprocessed_img</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">,</span><span class="se">\n</span><span class="s">Datatype: </span><span class="si">{</span><span class="n">preprocessed_img</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Image before preprocessing:
 [[[135 156 175]
  [125 148 166]
  [114 136 159]
  ...
  [ 26   5  12]
  [ 26   3  11]
  [ 27   4  12]]

 [[128 150 171]
  [115 140 160]
  [102 127 149]
  ...
  [ 28   7  14]
  [ 29   6  14]
  [ 30   7  15]]]...,
Shape: (512, 512, 3),
Datatype: &lt;dtype: 'uint8'&gt;

Image after preprocessing:
 [[[122.83163   146.17346   165.81633  ]
  [ 95.07653   122.122444  144.47958  ]
  [ 72.5051    106.994896  134.34694  ]
  ...
  [ 20.714308    2.3570995   3.9285717]
  [ 27.285715    6.285714   13.285714 ]
  [ 28.28575     5.2857494  13.285749 ]]

 [[ 88.65305   119.41326   140.41327  ]
  [ 74.59694   108.30102   133.02042  ]
  [ 75.2551    112.57143   141.91325  ]
  ...
  [ 26.857143    6.285671   11.040798 ]
  [ 30.061235    6.86222    16.795908 ]
  [ 31.688843    5.688843   16.688843 ]]]...,
Shape: (224, 224, 3),
Datatype: &lt;dtype: 'float32'&gt;
</code></pre></div></div>

<p>The data.AUTOTUNE function and the prefetch function work in tandem to utilize the multiple cores available to us and the gpu to efficiently process all of our images as quickly as possible.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Map preprocessing function to training data (and paralellize)
</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">map_func</span><span class="o">=</span><span class="n">preprocess_img</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="c1"># Shuffle train_data and turn it into batches and prefetch it (load it faster)
</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">).</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">).</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="c1"># Map prepreprocessing function to test data
</span><span class="n">test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">preprocess_img</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="c1"># Turn test data into batches (don't need to shuffle)
</span><span class="n">test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">).</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(&lt;PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int64)&gt;,
 &lt;PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int64)&gt;)
</code></pre></div></div>

<p>##Impementing Mixed Precision
Now lets implement Mixed precision. Here we can try to use flast 16 on some layers to improve speed and efficiency. It is only compatible with GPUâ€™s with a compute score of 7+.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">mixed_precision</span>
<span class="n">mixed_precision</span><span class="p">.</span><span class="n">set_global_policy</span><span class="p">(</span><span class="s">"mixed_float16"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5


INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5
</code></pre></div></div>

<h2 id="defining-our-feature-extraction-model">Defining our Feature-Extraction model</h2>
<p>Its time to define our model. Here is the order in which we define our layers:</p>

<ol>
  <li>InputLayer - takes input as an image[float 32]</li>
  <li>EfficientNetB0 - this is the main brains of our feature extraction</li>
  <li>Pooling Layer - to convert the output of efficient net into a feature vector</li>
  <li>Output Layer - gives an output as a probability distribution.</li>
</ol>

<p>We are ensuring that the layers of the EfficientNet is Freezed. This is to prevent any deviation of the already learned patterns. We need to also specify the float32 as output to the activation layer, as using the global float16 may cause issues with the softmax activation function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers.experimental</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="c1"># Create base model
</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">applications</span><span class="p">.</span><span class="n">EfficientNetB0</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">base_model</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># freeze base model layers
</span>
<span class="c1"># Create Functional model 
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"input_layer"</span><span class="p">)</span>
<span class="c1"># x = preprocessing.Rescaling(1./255)(x) EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below
</span><span class="n">x</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># set base_model to inference mode only
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"pooling_layer"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># want one output neuron per class 
# Separate activation of output layer so we can output float32 activations
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Activation</span><span class="p">(</span><span class="s">"softmax"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"softmax_float32"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

<span class="c1"># Compile the model
</span><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"sparse_categorical_crossentropy"</span><span class="p">,</span> <span class="c1"># Use sparse_categorical_crossentropy when labels are *not* one-hot
</span>              <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5
16711680/16705208 [==============================] - 0s 0us/step
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Printing a summary of the model
</span><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_layer (InputLayer)     [(None, 224, 224, 3)]     0         
_________________________________________________________________
efficientnetb0 (Functional)  (None, None, None, 1280)  4049571   
_________________________________________________________________
pooling_layer (GlobalAverage (None, 1280)              0         
_________________________________________________________________
dense (Dense)                (None, 101)               129381    
_________________________________________________________________
softmax_float32 (Activation) (None, 101)               0         
=================================================================
Total params: 4,178,952
Trainable params: 129,381
Non-trainable params: 4,049,571
_________________________________________________________________
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Checking the trainability and datatypes of the layers
</span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">dtype_policy</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input_layer True float32 &lt;Policy "float32"&gt;
efficientnetb0 False float32 &lt;Policy "mixed_float16"&gt;
pooling_layer True float32 &lt;Policy "mixed_float16"&gt;
dense True float32 &lt;Policy "mixed_float16"&gt;
softmax_float32 True float32 &lt;Policy "float32"&gt;
</code></pre></div></div>

<p>Creating our callback functions.</p>

<ol>
  <li>Tensorboard callback to save our training data.</li>
  <li>Model Checkpoint to save our best model.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create TensorBoard callback (already have "create_tensorboard_callback()" from a previous notebook)
</span><span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">create_tensorboard_callback</span>

<span class="c1"># Create ModelCheckpoint callback to save model's progress
</span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s">"model_checkpoints/cp.ckpt"</span> <span class="c1"># saving weights requires ".ckpt" extension
</span><span class="n">model_checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span>
                                                      <span class="n">monitor</span> <span class="o">=</span> <span class="s">'val_accuracy'</span><span class="p">,</span> <span class="c1"># save the model weights with best validation accuracy
</span>                                                      <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># only save the best weights
</span>                                                      <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># only save model weights (not whole model)
</span>                                                      <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>##Fitting the data on the Feature-Extraction model.</p>

<p>We are using the early_stopping callback to prevent any major overfitting and running the training for 3 epochs. We are testing on only 15 percent of the data to save time. Yet this is going to take a while processing over 100,000 images per epoch. So, Iâ€™m going to grab a cup of coffee.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history_101_food_classes_feature_extract</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> 
                                                     <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                                     <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
                                                     <span class="n">validation_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span>
                                                     <span class="n">validation_steps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.15</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)),</span>
                                                     <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">create_tensorboard_callback</span><span class="p">(</span><span class="s">"training_logs"</span><span class="p">,</span> 
                                                                                            <span class="s">"efficientnetb0_101_classes_all_data_feature_extract"</span><span class="p">),</span>
                                                                <span class="n">model_checkpoint</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Saving TensorBoard log files to: training_logs/efficientnetb0_101_classes_all_data_feature_extract/20210805-060548
Epoch 1/3
2368/2368 [==============================] - 192s 80ms/step - loss: 0.6993 - accuracy: 0.8149 - val_loss: 1.1099 - val_accuracy: 0.7021

Epoch 00001: val_accuracy did not improve from 0.71478
Epoch 2/3
2368/2368 [==============================] - 167s 70ms/step - loss: 0.6780 - accuracy: 0.8221 - val_loss: 1.1195 - val_accuracy: 0.7058

Epoch 00002: val_accuracy did not improve from 0.71478
Epoch 3/3
2368/2368 [==============================] - 171s 72ms/step - loss: 0.6579 - accuracy: 0.8262 - val_loss: 1.1418 - val_accuracy: 0.6989

Epoch 00003: val_accuracy did not improve from 0.71478
</code></pre></div></div>

<p>##Viewing the results, Saving our Feature-Extraction model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Evaluating our model on the test data
</span><span class="n">results_feature_extract_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
<span class="n">results_feature_extract_model</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>790/790 [==============================] - 55s 70ms/step - loss: 1.1396 - accuracy: 0.7001





[1.1395542621612549, 0.7001188397407532]
</code></pre></div></div>

<p>We got an accuracy of about 70% its close to our target ie <a href="https://arxiv.org/ftp/arxiv/papers/1606/1606.05675.pdf">Deepfoodâ€™s</a> original score of 77.4%. We still have another step to do, lets hope for the best.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Saving our model
</span><span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"101_food_feature_extract_mixedpred_model"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:tensorflow:Assets written to: 101_food_feature_extract_mixedpred_model/assets


INFO:tensorflow:Assets written to: 101_food_feature_extract_mixedpred_model/assets
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Loading our model into a seperate model
</span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="s">"101_food_feature_extract_mixedpred_model"</span><span class="p">)</span>
<span class="n">loaded_model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Checking if our layer types are accurate in loaded_model
</span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">loaded_model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">dtype_policy</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input_layer True float32 &lt;Policy "float32"&gt;
efficientnetb0 True float32 &lt;Policy "mixed_float16"&gt;
pooling_layer True float32 &lt;Policy "mixed_float16"&gt;
dense True float32 &lt;Policy "mixed_float16"&gt;
softmax_float32 True float32 &lt;Policy "float32"&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Evaulating on our loaded_model
</span><span class="n">test_eval</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>790/790 [==============================] - 49s 60ms/step - loss: 1.0722 - accuracy: 0.7116
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Seeing if the results match our actual model.
</span><span class="n">results_feature_extract_model</span> <span class="o">==</span> <span class="n">test_eval</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>False
</code></pre></div></div>

<p>##Making our Fine-Tuning model</p>

<p>To fine tune the model lets unfreeze the layers in our EfficientNet.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Are any of the layers in our model frozen?
</span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">loaded_model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
  <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># set all layers to trainable
</span>  <span class="k">print</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layer</span><span class="p">.</span><span class="n">dtype_policy</span><span class="p">)</span> <span class="c1"># make sure loaded model is using mixed precision dtype_policy ("mixed_float16")
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input_layer True float32 &lt;Policy "float32"&gt;
efficientnetb0 True float32 &lt;Policy "mixed_float16"&gt;
pooling_layer True float32 &lt;Policy "mixed_float16"&gt;
dense True float32 &lt;Policy "mixed_float16"&gt;
softmax_float32 True float32 &lt;Policy "float32"&gt;
</code></pre></div></div>

<p>Setting up Some more Callbacks for Fine-tuning</p>

<ol>
  <li>Tensorboard callback - to save our training data.</li>
  <li>Model Checkpoint - to save our best model.</li>
  <li>Early Stopping - to stop out training if our model is not improving</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs
</span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">"val_loss"</span><span class="p">,</span> <span class="c1"># watch the val loss metric
</span>                                                  <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># if val loss decreases for 3 epochs in a row, stop training
</span>
<span class="c1"># Create ModelCheckpoint callback to save best model during fine-tuning
</span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s">"fine_tune_checkpoints/"</span>
<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span>
                                                      <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                                      <span class="n">monitor</span><span class="o">=</span><span class="s">"val_loss"</span><span class="p">)</span>

<span class="n">reduce_lr</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">"val_loss"</span><span class="p">,</span>  
                                                 <span class="n">factor</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="c1"># multiply the learning rate by 0.2 (reduce by 5x)
</span>                                                 <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                                 <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># print out when learning rate goes down 
</span>                                                 <span class="n">min_lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loaded_model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"sparse_categorical_crossentropy"</span><span class="p">,</span> <span class="c1"># sparse_categorical_crossentropy for labels that are *not* one-hot
</span>                        <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span> <span class="c1"># 10x lower learning rate than the default
</span>                        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">])</span>
</code></pre></div></div>

<p>##Fitting the data on our Fine-Tuning model.</p>

<p>We are running the training for 100 epochs. The training will be stopped when the early stopping function is called. This is again going to take a while processing over 100,000 images per epoch. So, Iâ€™m going to grab another cup of coffee.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history_101_food_classes_all_data_fine_tune</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span>
                                                        <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="c1"># fine-tune for a maximum of 100 epochs
</span>                                                        <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
                                                        <span class="n">validation_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span>
                                                        <span class="n">validation_steps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.15</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)),</span> <span class="c1"># validation during training on 15% of test data
</span>                                                        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">create_tensorboard_callback</span><span class="p">(</span><span class="s">"training_logs"</span><span class="p">,</span> <span class="s">"efficientb0_101_classes_all_data_fine_tuning"</span><span class="p">),</span> <span class="c1"># track the model training logs
</span>                                                                   <span class="n">model_checkpoint</span><span class="p">,</span> <span class="c1"># save only the best model during training
</span>                                                                   <span class="n">early_stopping</span><span class="p">,</span> <span class="c1"># stop model after X epochs of no improvements
</span>                                                                   <span class="n">reduce_lr</span><span class="p">])</span> <span class="c1"># reduce the learning rate after X epochs of no improvements
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Saving TensorBoard log files to: training_logs/efficientb0_101_classes_all_data_fine_tuning/20210805-063310
Epoch 1/100
2368/2368 [==============================] - 313s 127ms/step - loss: 0.8116 - accuracy: 0.7748 - val_loss: 0.8895 - val_accuracy: 0.7524
INFO:tensorflow:Assets written to: fine_tune_checkpoints/assets


INFO:tensorflow:Assets written to: fine_tune_checkpoints/assets


Epoch 2/100
2368/2368 [==============================] - 296s 122ms/step - loss: 0.4660 - accuracy: 0.8666 - val_loss: 0.9664 - val_accuracy: 0.7505
Epoch 3/100
2368/2368 [==============================] - 290s 122ms/step - loss: 0.2495 - accuracy: 0.9273 - val_loss: 1.0183 - val_accuracy: 0.7532

Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.
Epoch 4/100
2368/2368 [==============================] - 290s 122ms/step - loss: 0.0796 - accuracy: 0.9808 - val_loss: 1.0898 - val_accuracy: 0.7773
</code></pre></div></div>

<p>##Viewing the results, Saving our Fine-Tuning model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loaded_model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"efficientnetb0_fine_tuned_101_classes_mixed_precision"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:tensorflow:Assets written to: efficientnetb0_fine_tuned_101_classes_mixed_precision/assets


INFO:tensorflow:Assets written to: efficientnetb0_fine_tuned_101_classes_mixed_precision/assets
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">final_score</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>790/790 [==============================] - 56s 70ms/step - loss: 1.0577 - accuracy: 0.7834
</code></pre></div></div>

<p>Yes! we did it! we beat the original <a href="https://arxiv.org/ftp/arxiv/papers/1606/1606.05675.pdf">DeepFoodâ€™s</a> score of 77.4%.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#saving the tensorboard data online - I have multiple models since Iyes tried a couple of ideas
</span><span class="err">!</span><span class="n">tensorboard</span> <span class="n">dev</span> <span class="n">upload</span> <span class="o">--</span><span class="n">logdir</span> <span class="p">.</span><span class="o">/</span><span class="n">training_logs</span> \
  <span class="o">--</span><span class="n">name</span> <span class="s">"Fine-tuning EfficientNetB0 on all Food101 Data"</span> \
  <span class="o">--</span><span class="n">description</span> <span class="s">"Training results for fine-tuning EfficientNetB0 on Food101 Data with learning rate 0.0001"</span> \
  <span class="o">--</span><span class="n">one_shot</span>
</code></pre></div></div>

<p><a href="https://tensorboard.dev/experiment/5coYXuxLQdyzAQAtJ0Nm8g/">Click here to view the analysis at tensorboard.</a></p>

<p>Downloading the files to my computer. these files should be available in the github repository.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="nb">zip</span> <span class="o">-</span><span class="n">r</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="nb">file</span><span class="p">.</span><span class="nb">zip</span> <span class="s">'101_food_feature_extract_mixedpred_model'</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  adding: 101_food_feature_extract_mixedpred_model/ (stored 0%)
  adding: 101_food_feature_extract_mixedpred_model/assets/ (stored 0%)
  adding: 101_food_feature_extract_mixedpred_model/variables/ (stored 0%)
  adding: 101_food_feature_extract_mixedpred_model/variables/variables.index (deflated 73%)
  adding: 101_food_feature_extract_mixedpred_model/variables/variables.data-00000-of-00001 (deflated 8%)
  adding: 101_food_feature_extract_mixedpred_model/saved_model.pb (deflated 92%)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">files</span>
<span class="n">files</span><span class="p">.</span><span class="n">download</span><span class="p">(</span><span class="s">"/content/file.zip"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="nb">zip</span> <span class="o">-</span><span class="n">r</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">file2</span><span class="p">.</span><span class="nb">zip</span> <span class="s">'efficientnetb0_fine_tuned_101_classes_mixed_precision'</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  adding: efficientnetb0_fine_tuned_101_classes_mixed_precision/ (stored 0%)
  adding: efficientnetb0_fine_tuned_101_classes_mixed_precision/assets/ (stored 0%)
  adding: efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/ (stored 0%)
  adding: efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/variables.index (deflated 78%)
  adding: efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/variables.data-00000-of-00001 (deflated 8%)
  adding: efficientnetb0_fine_tuned_101_classes_mixed_precision/saved_model.pb (deflated 92%)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">files</span><span class="p">.</span><span class="n">download</span><span class="p">(</span><span class="s">"/content/file2.zip"</span><span class="p">)</span>
</code></pre></div></div>

<p>##Conclusions</p>

<p>We acheived an accuracy of about 78.3%. With this we beat the score set by the original <a href="https://arxiv.org/ftp/arxiv/papers/1606/1606.05675.pdf">DeepFood</a> paper.</p>

<p>It feels great that we achieved our goal. But for those who were keen, our training accuracy was very high compared to our test accuracy. This is probably due to over fitting. On investigation I found out that the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet">EfficientNet</a> model was trainied upon the <a href="https://www.image-net.org/index.php">ImageNet</a> dataset. These images closely resemble our images. due to this our model ended up slightly overfitting.</p>

<p><strong>Nevertheless, we acheived our objective and thatâ€™s all that matters.</strong></p>

<p>Feel free to contact me on any of my socialâ€™s for anything!, Peace out!</p>
:ET